{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading raw data and preprocessing..\n",
      "Train size: 9596, test size 1066\n",
      "input shape:  (9596, 250)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Conv1D, MaxPooling1D, LSTM\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from functions import *\n",
    "\n",
    "\n",
    "# conf and preprocess -----------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# settings ---------------------\n",
    "# ------------------------------\n",
    "\n",
    "EMBEDDING = True\n",
    "TYPE = 'embedding' if EMBEDDING else 'standard'\n",
    "MODELPATH ='models/char-conv-' + TYPE + '-{epoch:02d}-{val_acc:.3f}-{val_loss:.3f}.hdf5'\n",
    "FILTERS = 500\n",
    "LR = 0.0001 if EMBEDDING else 0.00001\n",
    "\n",
    "\n",
    "# generate dataset -------------\n",
    "# ------------------------------\n",
    "\n",
    "data, table = load_processed_data(False, not EMBEDDING)\n",
    "print(\"input shape: \", np.shape(data.x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/2017-10-25 21:14:45_embedding\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 250, 16)           1120      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 250, 128)          74240     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 207,202.0\n",
      "Trainable params: 207,202.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model architecture ------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# input and embedding ----------\n",
    "# ------------------------------\n",
    "\n",
    "if EMBEDDING:\n",
    "\n",
    "    inputlayer = Input(shape=(250,))\n",
    "    network = Embedding(70, 16, input_length=250)(inputlayer)\n",
    "\n",
    "else:\n",
    "    inputlayer = Input(shape=(250 ,70))\n",
    "    network = inputlayer\n",
    "\n",
    "\n",
    "network = LSTM(128, return_sequences=True)(network)\n",
    "network = LSTM(128)(network)\n",
    "# fully connected --------------\n",
    "# ------------------------------\n",
    "\n",
    "#network = Flatten()(network)\n",
    "#network = Dense(128, activation='relu')(network)\n",
    "network = Dropout(0)(network)\n",
    "\n",
    "# output\n",
    "ypred = Dense(2, activation='softmax')(network)\n",
    "\n",
    "\n",
    "# training ----------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# callbacks --------------------\n",
    "# ------------------------------\n",
    "\n",
    "# tensorboard\n",
    "TB_DIR = 'logs/' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + '_' + TYPE\n",
    "\n",
    "os.makedirs(TB_DIR)\n",
    "tensorboard = TensorBoard(log_dir=TB_DIR)\n",
    "\n",
    "# early stopping and checkpoint\n",
    "estopping = EarlyStopping(monitor='val_acc', patience=1000)\n",
    "checkpoint = ModelCheckpoint(filepath=MODELPATH, save_best_only=True)\n",
    "\n",
    "# model-------------------------\n",
    "# ------------------------------\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputlayer, outputs=ypred)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(TB_DIR)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/500\n",
      "143s - loss: 0.7433 - acc: 0.5006 - val_loss: 0.7392 - val_acc: 0.4812\n",
      "Epoch 2/500\n",
      "136s - loss: 0.7250 - acc: 0.4967 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 3/500\n",
      "134s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 4/500\n",
      "135s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 5/500\n",
      "137s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 6/500\n",
      "134s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 7/500\n",
      "135s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 8/500\n",
      "138s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 9/500\n",
      "135s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 10/500\n",
      "137s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 11/500\n",
      "136s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 12/500\n",
      "135s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 13/500\n",
      "135s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 14/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 15/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 16/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 17/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 18/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 19/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 20/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 21/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 22/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 23/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 24/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 25/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 26/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 27/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 28/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 29/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 30/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 31/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 32/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 33/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 34/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 35/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 36/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 37/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 38/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 39/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 40/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 41/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 42/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 43/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 44/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 45/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 46/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 47/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 48/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 49/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 50/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 51/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 52/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 53/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 54/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 55/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 56/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 57/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 58/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 59/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 60/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 61/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 62/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 63/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 64/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 65/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 66/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 67/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 68/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 69/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 70/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 71/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 72/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 73/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 74/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 75/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 76/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 77/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 78/500\n",
      "134s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 79/500\n",
      "134s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 80/500\n",
      "134s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 81/500\n",
      "134s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 82/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 83/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 84/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 85/500\n",
      "134s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 86/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 87/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 88/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 89/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 90/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 91/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 92/500\n",
      "131s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 93/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 94/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 95/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 96/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 97/500\n",
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 98/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 99/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 100/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 101/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 102/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 103/500\n",
      "132s - loss: 0.7236 - acc: 0.4973 - val_loss: 0.7393 - val_acc: 0.4812\n",
      "Epoch 104/500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# fit and run ------------------\n",
    "# ------------------------------\n",
    "try:\n",
    "    hist = model.fit(data.x_train,\n",
    "                     data.y_train,\n",
    "                     validation_data=(data.x_test, data.y_test),\n",
    "                     epochs=500,\n",
    "                     batch_size=50,\n",
    "                     shuffle=False,\n",
    "                     verbose=2,\n",
    "                     callbacks=[checkpoint, estopping, tensorboard])\n",
    "\n",
    "except KeyboardInterrupt:    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
